# Text-Summarization-for-Indo-Aryan-Languages-using-Self-Attention-Mechanism

When it comes to extracting pertinent information from lengthy textual materials, text summary is essential. Nonetheless, there are very few language models available for use with texts written in regional or native tongues. Regional languages frequently face issues such as a lack of digital material, resource constraints, linguistic inflections, and structural variance. This work attempts to fill up those similar gaps so that we can model text summarizers in regional languages. In this work, we investigate the efficacy of self-attention mechanisms for abstractive text summarization in two Indo-Aryan languages, Gujarati and Hindi, by particularly utilizing a Conventional Transformer. Our methodology entails using the ILSUM dataset to train a specially constructed Transformer model. Articles, headlines, and summaries are used to curate corpora for the Indian Language Summarization projectÂ (ILSUM).

</br>
<img width="316" height="476" alt="image" src="https://github.com/user-attachments/assets/2f636ec7-359e-4e0a-a758-133bde4eb75b" />


#### Paper Citation: 
Hadawle, S., Kotkar, P., Bhatia, O., Dongre, S., Singh, A.R. (2025). Text Summarization of Indo-Aryan Languages Using Self-attention Mechanism. In: Kalam, A., Mekhilef, S., Williamson, S.S. (eds) Innovations in Electrical and Electronics Engineering. ICEEE 2024. Lecture Notes in Electrical Engineering, vol 1295. Springer, Singapore. https://doi.org/10.1007/978-981-97-9112-5_29
